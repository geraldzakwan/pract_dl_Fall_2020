{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `numpy` and `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define needed functions related to generating and plotting 2D data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate $n$ points of $(x1, x2)$, with range: $low \\leq x1 \\leq x2 \\leq high$\n",
    "def generate_2D_points(low, high, n_points, seed=None):\n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    x1_arr = np.random.uniform(low, high, n_points)\n",
    "    x2_arr = np.random.uniform(low, high, n_points)\n",
    "\n",
    "    class_1_data, class_2_data = ([], [])\n",
    "\n",
    "    for x1, x2 in zip(x1_arr, x2_arr):\n",
    "        if x1 > x2:\n",
    "            class_1_data.append((x1, x2))\n",
    "        else:\n",
    "            class_2_data.append((x1, x2))\n",
    "\n",
    "    class_1_data, class_2_data = (np.array(class_1_data), np.array(class_2_data))\n",
    "    \n",
    "    return class_1_data, class_2_data\n",
    "\n",
    "# Function to plot 2D points, color coded based on class (two classes)\n",
    "def plot_2D_points(class_1_data, class_2_data, class_3_data=[]):\n",
    "    plt.plot(class_1_data[:, 0], class_1_data[:, 1], 'o', color='green', label='Class 1')\n",
    "\n",
    "    plt.plot(class_2_data[:, 0], class_2_data[:, 1], 'o', color='blue', label='Class 2')\n",
    "    \n",
    "    if len(class_3_data) > 0:\n",
    "        plt.plot(class_3_data[:, 0], class_3_data[:, 1], 'o', color='red', label='Removed Points')\n",
    "        \n",
    "    ax = plt.gca()\n",
    "    ax.set(xlabel = 'x1', ylabel = 'x2')\n",
    "\n",
    "    plt.title('2D Scatter Plot for points (x1, x2) in Class 1 (Green) and Class 2 (Blue)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate $20$ points in the `unit square of positive quadrant`, i.e. `Quadrant I` with range: $0 \\leq x1 \\leq x2 \\leq 1$. This is for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1_train_data, class_2_train_data = generate_2D_points(0, 1, 20, 15)\n",
    "\n",
    "plot_2D_points(class_1_train_data, class_2_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate $1000$ points in the `unit square of positive quadrant`, i.e. `Quadrant I` with range: $0 \\leq x1 \\leq x2 \\leq 1$. This is for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1_test_data, class_2_test_data = generate_2D_points(0, 1, 1000, 15)\n",
    "\n",
    "plot_2D_points(class_1_test_data, class_2_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build X and y for train and test, check if the shape matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack([class_1_train_data.T, class_2_train_data.T])\n",
    "X_test = np.hstack([class_1_test_data.T, class_2_test_data.T])\n",
    "\n",
    "assert X_train.shape == (2, 20)\n",
    "assert X_test.shape == (2, 1000)\n",
    "\n",
    "y_train = np.concatenate((np.full(len(class_1_train_data), float(1.0)), np.full(len(class_2_train_data), float(-1.0))), axis=None)\n",
    "y_test = np.concatenate((np.full(len(class_1_test_data), float(1.0)), np.full(len(class_2_test_data), float(-1.0))), axis=None)\n",
    "\n",
    "assert y_train.shape == (20,)\n",
    "assert y_test.shape == (1000,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the data first so later in each minibatch, the Perceptron sees data from both labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X, y):\n",
    "    idxes = np.arange(X.shape[1])\n",
    "    np.random.shuffle(idxes)\n",
    "    \n",
    "    return np.array([X[0][idxes], X[1][idxes]]), y[idxes]\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "X_test, y_test = shuffle(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Perceptron related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return label given output using sign function\n",
    "# x should be an NDARRAY, not a scalar\n",
    "def sign(x):\n",
    "    return np.array([1.0 if val >= 0 else -1.0 for val in x])\n",
    "\n",
    "# Predict label using sign activation\n",
    "def predict(W, X):\n",
    "    return sign(np.dot(W.T, X))\n",
    "\n",
    "# Init weights randomly using normal distribution\n",
    "def init_weights(w_ranges, seed=None):\n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    w1 = np.random.uniform(w_ranges[0][0], w_ranges[0][1], size=None)\n",
    "    w2 = np.random.uniform(w_ranges[1][0], w_ranges[1][1], size=None)\n",
    "    \n",
    "    return np.array([w1, w2])\n",
    "\n",
    "# X should be in the shape of (n_features, n_data)\n",
    "# y should be in the shape of (n_data,)\n",
    "# Function to compute loss\n",
    "def forward_propagate(W, X, y, algo):\n",
    "    n_data = X.shape[1]\n",
    "    \n",
    "    # a equals 0 for Perceptron Criterion and 1 for Hinge-Loss\n",
    "    if algo == 'Perceptron criterion':\n",
    "        a = np.zeros(n_data)\n",
    "    \n",
    "    elif algo == 'Hinge loss':\n",
    "        a = np.full(n_data, 1)\n",
    "        \n",
    "    else:\n",
    "        print('Invalid algorithm')\n",
    "        assert False\n",
    "    \n",
    "    outputs = np.dot(W.T, X)\n",
    "    assert outputs.shape == (n_data,)\n",
    "    \n",
    "    # IMPORTANT: Do ELEMENT-WISE multiplication for y against the output\n",
    "    diffs = a - np.multiply(y, outputs)\n",
    "    assert diffs.shape == (n_data,)\n",
    "    \n",
    "    losses = np.maximum(np.zeros(n_data), diffs)\n",
    "    assert losses.shape == (n_data,)\n",
    "    \n",
    "    # IMPORTANT: Don't forget to divide the losses \n",
    "    # by the total number of data so that the updates\n",
    "    # are not EXTREME\n",
    "    losses = np.divide(losses, n_data)\n",
    "    assert losses.shape == (n_data,)\n",
    "\n",
    "    return outputs, losses\n",
    "\n",
    "# X should be in the shape of (n_features, n_data)\n",
    "# y should be in the shape of (n_data,)\n",
    "# Function to update weights\n",
    "def back_propagate(W, X, y, outputs, losses, learning_rate):\n",
    "    y_preds = sign(outputs)\n",
    "    assert y_preds.shape == (X.shape[1],)\n",
    "    \n",
    "    diff = np.subtract(y, y_preds)\n",
    "    assert diff.shape == (X.shape[1],)\n",
    "    \n",
    "    # Update only when loss > 0\n",
    "    for i, loss in enumerate(losses):\n",
    "        if loss > 0.0:\n",
    "            # IMPORTANT: Slice the second axis\n",
    "            curr_X = X[:, i]\n",
    "            assert curr_X.shape == (X.shape[0],)\n",
    "            \n",
    "            update_values = np.multiply(learning_rate, np.multiply(curr_X, diff[i]))\n",
    "            assert update_values.shape == (X.shape[0],)\n",
    "\n",
    "            W = np.add(W, update_values)\n",
    "            assert W.shape == (X.shape[0],)\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop, stops when error is zero or when max_iter is reached\n",
    "def train(X, y, algo, learning_rate, max_iter, w_ranges, seed, debug):\n",
    "    W = init_weights(w_ranges, seed)\n",
    "    \n",
    "    print('Initial Weights:')\n",
    "    print('w1: {:.2f}, w2: {:.2f}'.format(W[0], W[1]))\n",
    "    print('-----------------------')\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    while True:\n",
    "        outputs, losses = forward_propagate(W, X_train, y_train, algo)\n",
    "        W = back_propagate(W, X_train, y_train, outputs, losses, learning_rate)\n",
    "        \n",
    "        avg_losses = np.mean(losses)\n",
    "        misc = compute_performance(W, X_train, y_train, 'misc')\n",
    "        \n",
    "        if algo == 'Perceptron criterion':\n",
    "            if misc == 0:\n",
    "                break\n",
    "                \n",
    "        elif algo == 'Hinge loss':\n",
    "            if avg_losses == 0.0:\n",
    "                break\n",
    "                \n",
    "        else:\n",
    "            print('Invalid algorithm')\n",
    "            assert False\n",
    "        \n",
    "        i = i + 1\n",
    "        \n",
    "        if i < max_iter:\n",
    "            if debug[0]:\n",
    "                if i % debug[1] == 0:\n",
    "                    print('Iteration: {}'.format(i))\n",
    "                    print('Number of misclassified examples: {}'.format(misc))\n",
    "                    print('w1: {:.2f}, w2: {:.2f}'.format(W[0], W[1]))\n",
    "                    print('-----------------------')\n",
    "        \n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    print('{} stops at iteration: {}'.format(algo, i))\n",
    "    print('Number of misclassified examples: {}'.format(misc))\n",
    "    print('w1: {:.2f}, w2: {:.2f}'.format(W[0], W[1]))\n",
    "    print('-----------------------')\n",
    "    \n",
    "    return W\n",
    "   \n",
    "# Call predict to get labels and then compute test accuracy\n",
    "def compute_performance(W, X, y, ret_type):\n",
    "    y_preds = predict(W, X)\n",
    "    \n",
    "    verdict = [1 if val1 == val2 else 0 for val1, val2 in zip(y_preds, y)]\n",
    "    \n",
    "    if ret_type == 'misc':\n",
    "        return len(y) - np.sum(verdict)\n",
    "    elif ret_type == 'acc':\n",
    "        return np.sum(verdict) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for Perceptron criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = 'Perceptron criterion'\n",
    "learning_rate = 0.1\n",
    "n_iters = 100 # Stop when loss equals zero or reach n_iters\n",
    "debug = (False, None)\n",
    "\n",
    "# Initial value range for (w1, w2), uniform random from -0.1 to 0.1\n",
    "w_ranges = ((-0.1, 0.1), (-0.1, 0.1)) \n",
    "seed = 8\n",
    "\n",
    "W_pc = train(X_train, y_train, algo, learning_rate, n_iters, w_ranges, seed, debug)\n",
    "test_acc = compute_performance(W_pc, X_test, y_test, 'acc')\n",
    "\n",
    "print('Test accuracy using Perceptron criterion is: {}'.format(test_acc))\n",
    "print('-----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for Hinge losss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = 'Hinge loss'\n",
    "learning_rate = 0.25\n",
    "n_iters = 100 # Stop when loss equals zero or reach n_iters\n",
    "debug = (False, None)\n",
    "\n",
    "# Initial value range for (w1, w2), uniform random from -0.1 to 0.1\n",
    "w_ranges = ((-0.1, 0.1), (-0.1, 0.1))\n",
    "seed = 8\n",
    "\n",
    "W_hl = train(X_train, y_train, algo, learning_rate, n_iters, w_ranges, seed, debug)\n",
    "test_acc = compute_performance(W_hl, X_test, y_test, 'acc')\n",
    "\n",
    "print('Test accuracy using Hinge loss is: {}'.format(test_acc))\n",
    "print('-----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    "\n",
    "In which case do you obtain better accuracy and why?\n",
    "\n",
    "Answer:\n",
    "\n",
    "I obtained better test accuracy using Hinge loss (0.974) compared to Perceptron criterion (0.952). \n",
    "\n",
    "We know that Perceptron criterion only updates the weights when there is a discrepancy between the predicted sign, i.e. $\\hat{y} = sign(\\bar{W}.\\bar{X})$, and the true sign, i.e. $y$. Thus, the decision boundary produces by the Perceptron doesn't really maximize the margin between the two classes: $y=-1$ and $y=1$.\n",
    "\n",
    "Meanwhile, Hinge loss also updates the weights when there is a discrepancy between the predicted output, i.e. $\\bar{W}.\\bar{X}$, and the actual output, denoted by $1 - y(\\bar{W}.\\bar{X})$. Thus, it maximizes the margin between the two classes and is more robust to the actual data/test set than Perceptron criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    "\n",
    "In which case do you think that the classification of the same 1000 test instances will not change significantly by using a different set of 20 training points? (2)\n",
    "\n",
    "Answer:\n",
    "\n",
    "In Hinge loss case, with the similar reasoning as part 1.3. We know that Hinge loss always maximizes the margin between the two classes. \n",
    "\n",
    "Since Hinge loss always maximize the margin, the resulting decision boundary will remain similar. Meanwhile, in the Perceptron case, the resulting decision boundary very much depends on the location of the training data for each class. \n",
    "\n",
    "Take a look again at the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2D_points(class_1_train_data[[1, 3, 4]], class_2_train_data, class_1_train_data[[0, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we remove the two points near the true decision boundary $x_1 - x_2 = 0$ from Class 1, denoted by the red dots. In that case, we can infer that the Perceptron might have a decision boundary that is close towards the green region, say something like $2x_1 - 3x_2 = 0$. This decision boundary is not close to the true decision boundary of $x_1 - x_2 = 0$. Thus, Perceptron criterion classification will likely to change significantly given different training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Weight Initialization, Dead Neurons, Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we are using `Tensorflow 2.3`, otherwise some functions like `tf.keras.initializers.HeNormal` are not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codebase from https://github.com/Intoli/intoli-article-materials/tree/master/articles/neural-network-initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParamsDefault\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_axes_it(n_plots, n_cols=3, enumerate=False, fig=None):\n",
    "    \"\"\"\n",
    "    Iterate through Axes objects on a grid with n_cols columns and as many\n",
    "    rows as needed to accommodate n_plots many plots.\n",
    "    Args:\n",
    "        n_plots: Number of plots to plot onto figure.\n",
    "        n_cols: Number of columns to divide the figure into.\n",
    "        fig: Optional figure reference.\n",
    "    Yields:\n",
    "        n_plots many Axes objects on a grid.\n",
    "    \"\"\"\n",
    "    n_rows = n_plots / n_cols + int(n_plots % n_cols > 0)\n",
    "\n",
    "    if not fig:\n",
    "        default_figsize = rcParamsDefault['figure.figsize']\n",
    "        fig = plt.figure(figsize=(\n",
    "            default_figsize[0] * n_cols,\n",
    "            default_figsize[1] * n_rows\n",
    "        ))\n",
    "\n",
    "    for i in range(1, n_plots + 1):\n",
    "        ax = plt.subplot(n_rows, n_cols, i)\n",
    "        yield ax\n",
    "\n",
    "\n",
    "def create_mlp_model(\n",
    "    n_hidden_layers,\n",
    "    dim_layer,\n",
    "    input_shape,\n",
    "    n_classes,\n",
    "    kernel_initializer,\n",
    "    bias_initializer,\n",
    "    activation,\n",
    "):\n",
    "    \"\"\"Create Multi-Layer Perceptron with given parameters.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dim_layer, input_shape=input_shape, kernel_initializer=kernel_initializer,\n",
    "                    bias_initializer=bias_initializer))\n",
    "    for i in range(n_hidden_layers):\n",
    "        model.add(Dense(dim_layer, activation=activation, kernel_initializer=kernel_initializer,\n",
    "                        bias_initializer=bias_initializer))\n",
    "    model.add(Dense(n_classes, activation='softmax', kernel_initializer=kernel_initializer,\n",
    "                    bias_initializer=bias_initializer))\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_model(model):\n",
    "    model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\n",
    "                  optimizer=tensorflow.keras.optimizers.RMSprop(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_activations(model, x, mode=0.0):\n",
    "    \"\"\"Extract activations with given model and input vector x.\"\"\"\n",
    "    outputs = [layer.output for layer in model.layers]\n",
    "    activations = K.function([model.input], outputs)\n",
    "    output_elts = activations([x, mode])\n",
    "    return output_elts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the main routine from the original source code into some functions below, for reusability and readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mnist_data(flatten=False, newaxis=False):\n",
    "    # Load and prepare MNIST dataset.\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    data_dim = 784\n",
    "    n_train = len(X_train)\n",
    "    n_test = len(X_test)\n",
    "    n_classes = len(np.unique(y_test))\n",
    "    \n",
    "    if flatten:\n",
    "        X_train = X_train.reshape(n_train, data_dim).astype('float32')[:n_train]\n",
    "        X_test = X_test.reshape(n_test, data_dim).astype('float32')[:n_test]\n",
    "        \n",
    "        if newaxis:\n",
    "            X_train = X_train[:, :, np.newaxis]\n",
    "            X_test = X_test[:, :, np.newaxis]\n",
    "    else:\n",
    "        X_train = X_train.astype('float32')[:n_train]\n",
    "        X_test = X_test.astype('float32')[:n_test]\n",
    "        \n",
    "        if newaxis:\n",
    "            X_train = X_train[:, :, :, newaxis]\n",
    "            X_test = X_test[:, :, :, np.newaxis]\n",
    "        \n",
    "    X_train /= 255.0\n",
    "    X_test /= 255.0\n",
    "\n",
    "    y_train = tensorflow.keras.utils.to_categorical(y_train, n_classes)\n",
    "    y_test = tensorflow.keras.utils.to_categorical(y_test, n_classes)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, n_classes, data_dim\n",
    "\n",
    "def train_model(params, X):\n",
    "    activation_data = []\n",
    "    \n",
    "    if params['kernel_init'] == 'random_normal':\n",
    "        print('Using Random Normal')\n",
    "        kernel_init = initializers.RandomNormal(mean=0.0, stddev=params['stddev'], seed=params['seed'])\n",
    "    elif params['kernel_init'] == 'glorot_normal':\n",
    "        print('Using Glorot Normal')\n",
    "        kernel_init = initializers.GlorotNormal(seed=params['seed'])\n",
    "    elif params['kernel_init'] == 'he_normal':\n",
    "        print('Using He Normal')\n",
    "        kernel_init = initializers.HeNormal(seed=params['seed'])\n",
    "    else:\n",
    "        print('Invalid initializer')\n",
    "        assert False\n",
    "\n",
    "    model = create_mlp_model(\n",
    "        params['n_hidden_layers'],\n",
    "        params['dim_layer'],\n",
    "        (params['data_dim'],),\n",
    "        params['n_classes'],\n",
    "        kernel_init,\n",
    "        params['bias_init'],\n",
    "        params['activation']\n",
    "    )\n",
    "    \n",
    "    compile_model(model)\n",
    "    \n",
    "    output_elts = get_activations(model, X)\n",
    "    \n",
    "    n_layers = len(model.layers)\n",
    "    \n",
    "    i_output_layer = n_layers - 1\n",
    "\n",
    "    for i, out in enumerate(output_elts[:-1]):\n",
    "        if i > 0 and i != i_output_layer:\n",
    "            for out_i in out.ravel()[::20]:\n",
    "                activation_data.append([i, stddev, out_i])\n",
    "                \n",
    "    return activation_data   \n",
    "\n",
    "def plot_activations_random_normal(df, sigmas, activation):\n",
    "    # Plot previously saved activations from the 5 hidden layers\n",
    "    # using different initialization schemes.\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "\n",
    "    axes = grid_axes_it(len(sigmas), 1, fig=fig)\n",
    "\n",
    "    for i, sig in enumerate(sigmas):\n",
    "        ax = next(axes)\n",
    "        ddf = df[df['Standard Deviation'] == sig]\n",
    "        sns.violinplot(x='Hidden Layer', y='Output', data=ddf, ax=ax, scale='count', inner=None)\n",
    "\n",
    "        ax.set_title('Weights Drawn from $N(\\mu = 0, \\sigma = {:.2f})$ using {} activation function in hidden layers'.format(sig, activation), fontsize=13)\n",
    "\n",
    "        ax.set_xlabel(\"Hidden Layer\")\n",
    "\n",
    "        if i == len(sigmas) // 2: \n",
    "            ax.set_ylabel(\"Neuron Activations\")\n",
    "\n",
    "        if i != len(sigmas) - 1:\n",
    "            ax.set_xticklabels(())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_activations_glorot_and_he_normal(df, init, activation):\n",
    "    # Plot previously saved activations from the 5 hidden layers\n",
    "    # using different initialization schemes.\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "\n",
    "    axes = grid_axes_it(1, 1, fig=fig)\n",
    "    ax = next(axes)\n",
    "    \n",
    "    sns.violinplot(x='Hidden Layer', y='Output', data=df, ax=ax, scale='count', inner=None)\n",
    "\n",
    "    ax.set_title('Weights Drawn from {} Uniform using {} activation function in hidden layers'.format(init, activation), fontsize=13)\n",
    "\n",
    "    ax.set_xlabel(\"Hidden Layer\")\n",
    "    ax.set_ylabel(\"Neuron Activations\")\n",
    "    ax.set_xticklabels(())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanishing Gradients Phenomenon using Random Normal Initialization for Tanh and Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train and test data from MNIST\n",
    "X_train, y_train, X_test, y_test, n_classes, data_dim = generate_mnist_data(flatten=True, newaxis=False)\n",
    "\n",
    "# MLP Params in relation to the dataset\n",
    "params = {\n",
    "    'n_hidden_layers': 5,\n",
    "    'dim_layer': 100,\n",
    "    'data_dim': data_dim,\n",
    "    'n_classes': n_classes,\n",
    "    'kernel_init': 'random_normal',\n",
    "    'bias_init': 'zeros',\n",
    "    'seed': 10\n",
    "}\n",
    "\n",
    "# Run the data through a few MLP models and save the activations from\n",
    "# each layer into a Pandas DataFrame.\n",
    "activation_data = []\n",
    "activations = ['Tanh', 'Sigmoid']\n",
    "sigmas = [1.0]\n",
    "\n",
    "for activation in activations:\n",
    "    print(activation + ' Hidden Layer Neuron Activation Plots')\n",
    "    print('-------------------')\n",
    "    \n",
    "    params['activation'] = activation.lower()\n",
    "    \n",
    "    for stddev in sigmas:\n",
    "        params['stddev'] = stddev\n",
    "        \n",
    "        for activation_datum in train_model(params, X_train):\n",
    "            activation_data.append(activation_datum)\n",
    "\n",
    "    df = pd.DataFrame(activation_data, columns=['Hidden Layer', 'Standard Deviation', 'Output'])\n",
    "\n",
    "    plot_activations_random_normal(df, sigmas, activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing Vanishing Gradients Phenomenon using Glorot Uniform Initialization for Tanh and Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update 'kernel_init', i.e. weight initializer, in MLP Params\n",
    "params['kernel_init'] = 'glorot_normal' \n",
    "\n",
    "# Empty activation data\n",
    "activation_data = []\n",
    "\n",
    "# Add ReLU activation function\n",
    "activations.append('ReLU')\n",
    "\n",
    "for activation in activations:\n",
    "    print(activation + ' Hidden Layer Neuron Activation Plots')\n",
    "    print('-------------------')\n",
    "    \n",
    "    params['activation'] = activation.lower()\n",
    "        \n",
    "    for activation_datum in train_model(params, X_train):\n",
    "        activation_data.append(activation_datum)\n",
    "\n",
    "    df = pd.DataFrame(activation_data, columns=['Hidden Layer', 'Standard Deviation', 'Output'])\n",
    "    df.drop(['Standard Deviation'], axis=1)\n",
    "\n",
    "    plot_activations_glorot_and_he_normal(df, 'Glorot Normal', activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're using He initializers with ReLU activation function in hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update 'kernel_init', i.e. weight initializer, in MLP Params\n",
    "params['kernel_init'] = 'he_normal' \n",
    "\n",
    "# Empty activation data\n",
    "activation_data = []\n",
    "\n",
    "# Only use ReLU\n",
    "activations = ['ReLU']\n",
    "\n",
    "for activation in activations:\n",
    "    print(activation + ' Hidden Layer Neuron Activation Plots')\n",
    "    print('-------------------')\n",
    "    \n",
    "    params['activation'] = activation.lower()\n",
    "        \n",
    "    for activation_datum in train_model(params, X_train):\n",
    "        activation_data.append(activation_datum)\n",
    "\n",
    "    df = pd.DataFrame(activation_data, columns=['Hidden Layer', 'Standard Deviation', 'Output'])\n",
    "    df.drop(['Standard Deviation'], axis=1)\n",
    "\n",
    "    plot_activations_glorot_and_he_normal(df, 'He Normal', activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate dataset related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_x(x):\n",
    "    return np.multiply(x, np.sin(np.multiply(x, 5.0)))\n",
    "\n",
    "def generate_dataset(n_data):\n",
    "    X = np.random.uniform(-1.0 * np.sqrt(7), np.sqrt(7), n_data)\n",
    "    assert X.shape == (n_data,)\n",
    "\n",
    "    y = f_x(X)\n",
    "    assert y.shape == (n_data,)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def train_test_split(X, y, train_ratio):\n",
    "    n_data = len(X)\n",
    "    assert n_data == len(y)\n",
    "    \n",
    "    idx_split = int(train_ratio * n_data)\n",
    "    \n",
    "    return X[:idx_split], y[:idx_split], X[idx_split:], y[idx_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable parallel training, each model training will now take only 0.1 fraction of the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.compat.v1.keras.backend import set_session\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.1 # set 0.3 to what you want\n",
    "# set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define network and its training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_relu_model(params):\n",
    "    # Create the model as sequential\n",
    "    model = Sequential()\n",
    "\n",
    "    # First hidden layer\n",
    "    model.add(Dense(\n",
    "        params['n_hidden_units'], \n",
    "        input_shape=(params['feature_vector_length'],), \n",
    "        activation=params['activation'],\n",
    "        kernel_initializer=params['kernel_init'],\n",
    "        bias_initializer=params['bias_init']\n",
    "    ))\n",
    "\n",
    "    # The rest of the hidden layers\n",
    "    for i in range(params['n_hidden_layers']-1):\n",
    "        model.add(Dense(params['n_hidden_units'], activation=params['activation']))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile, use MAE loss, Adam optimizer\n",
    "    model.compile(loss=params['loss'], optimizer=params['optimizer'], metrics=params['metrics'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "def train_dying_relu(params):\n",
    "    collapse_count = 0\n",
    "    \n",
    "    if params['timed']:\n",
    "        start = time.time()\n",
    "    \n",
    "    for i in range(0, params['n_experiments']):\n",
    "        X, y = generate_dataset(params['n_data'])\n",
    "        \n",
    "        X_train, y_train, X_test, y_test = train_test_split(X, y, 0.8)\n",
    "        \n",
    "        # Make sure weights are different every time we do the independent experiment\n",
    "        model = build_relu_model(params)\n",
    "        \n",
    "        # Train using minibatches\n",
    "        model.fit(X_train, y_train, epochs=params['epochs'], batch_size=params['batch_size'], verbose=params['verbose'])\n",
    "        \n",
    "        # Check if collapsed\n",
    "        if is_collapsed(model, X_test, params['threshold']):\n",
    "            collapse_count += 1\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Experiment number: {}'.format(i+1))\n",
    "            \n",
    "            if params['timed']:\n",
    "                end = time.time()\n",
    "\n",
    "                print('Elapsed time: {}'.format(end - start))\n",
    "                \n",
    "                start = time.time()\n",
    "        \n",
    "    return collapse_count / params['n_experiments'] * 100\n",
    "\n",
    "# Define functions to check if network collapses\n",
    "def is_collapsed(network, X_test, threshold):\n",
    "    y_preds = network.predict(X_test).flatten()\n",
    "    \n",
    "    if np.var(y_preds) <= threshold:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configs\n",
    "params = {\n",
    "    'feature_vector_length': 1,\n",
    "    'n_hidden_layers': 10,\n",
    "    'n_hidden_units': 2,\n",
    "    'batch_size': 64,\n",
    "    'activation': ReLU(),\n",
    "    'kernel_init': initializers.HeNormal(),\n",
    "    'bias_init': initializers.Zeros(),\n",
    "    'loss': 'mean_absolute_error',\n",
    "    'optimizer': 'adam',\n",
    "    'metrics': 'mean_squared_error',\n",
    "    'epochs': 10,\n",
    "    'verbose': 0,\n",
    "    'timed': True,\n",
    "    'n_data': 3000,\n",
    "    'n_experiments': 1000,\n",
    "    'threshold': 1e-4 # To determine if network collapses\n",
    "}\n",
    "\n",
    "collapse_percentage = train_dying_relu(params)\n",
    "\n",
    "print('Collapse percentage for ReLU is: {:.2f}%'.format(collapse_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just change the `activation` to `LeakyReLU` with `alpha=0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes only the activation\n",
    "params['activation'] = LeakyReLU(alpha=0.01)\n",
    "\n",
    "leaky_collapse_percentage = train_dying_relu(params)\n",
    "\n",
    "print('Collapse percentage for Leaky ReLU with $N(\\alpha = 0.01$ is: {:.2f}%'.format(leaky_collapse_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the collapse percentage is now reduced to ...% from ...%. So, Leaky ReLU does help in preventing dying neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 - Batch Normalization, Dropout, MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    "\n",
    "Explain the terms co-adaptation and internal covariance-shift. Use examples if needed. You may need to refer to two papers mentioned below to answer this question.\n",
    "\n",
    "Answer:\n",
    "\n",
    "1. Co-Adaptation: In deep learning/neural networks, Co-Adaptation happens when different hidden units have similar behavior so that they might cancel each other out [1]. It is not good because it is computationally inefficient and we desire different hidden units to function differently, i.e. to detect independent features. For example, suppose we have two hidden units, each have four incoming weights from the previous units/nodes, denoted by $[w_{11}, ..., w_{14}]$ and $[w_{21}, ..., w_{24}]$ below: $$w_{1}=[−0.82,1.23,−2.34,0.82]$$ $$w_2=[−0.82,−1.20,2.29,0.82]$$ We can see that $w_{12}$ cancels $w_{22}$ and $w_{13}$ cancels $w_{23}$ since their absolute values are similar but they are of different signs. In this case, we actually only need 2 previous nodes to approximate the function, but we have 4 nodes instead. This is indesirable because it means those four weights have high coupling with each other. Not only inefficient, but in the case of one the weights is rendered invalid (e.g. the server crashes and loses its value), then the other couldn't take over the job since they represent more or less the same feature and the network fails, i.e. producing very different output. Dropout mechanism solves this problem.\n",
    "\n",
    "2. Internal Covariance-Shift: In deep learning/neural networks, Internal Covariance-Shift is defined as the change in the distribution of network activations due to the change in network parameters/weights during training [2]. We know that the output of the first layer is fed into the second layer and so on. Thus, when the weights are updated, the distribution of inputs to subsequent layers will follow suit. Since hidden layers will try to cope with the new distribution every iteration, then the training will be slower, particularly when the statistical distribution of the input to the networks is drastically different from the previous one. For example, consider MLP that is trained with MNIST data in grayscale images. If we now try to apply this MLP to colored handwritten digits, then it will not perform. Both are handwritten digits but obviously with different statistics (mean and variance). To sum up, if a neural network learns some mapping $X \\rightarrow Y$ and the distribution of $X$ changes, then we might need to retrain the network by aligning the distribution of $X$ with the distribution of $Y$. Batch Normalization solves this problem.\n",
    "\n",
    "Reference:\n",
    "\n",
    "1. https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\n",
    "2. https://arxiv.org/pdf/1502.03167.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Le-Net 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.layers import Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lenet_5(batch_norm_params, dropout_params):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add the first convolution layer\n",
    "    model.add(Conv2D(\n",
    "        filters = 20,\n",
    "        kernel_size = (5, 5),\n",
    "        padding = \"same\",\n",
    "        input_shape = (28, 28, 1)))\n",
    "    \n",
    "    if batch_norm_params['prev_to_act'] and batch_norm_params['input_layer']:\n",
    "        if batch_norm_params['input_norm_type'] == 'batch_norm':\n",
    "            model.add(BatchNormalization())\n",
    "        \n",
    "    if dropout_params['prev_to_act'] and dropout_params['input_layer']:\n",
    "        model.add(Dropout(dropout_params['input_layer_rate']))\n",
    "\n",
    "    # Add a ReLU activation function\n",
    "    model.add(Activation(\n",
    "        activation = \"relu\"))\n",
    "    \n",
    "    if not batch_norm_params['prev_to_act'] and batch_norm_params['input_layer']:\n",
    "        if batch_norm_params['input_norm_type'] == 'batch_norm':\n",
    "            model.add(BatchNormalization())\n",
    "        \n",
    "    if not dropout_params['prev_to_act'] and dropout_params['input_layer']:\n",
    "        model.add(Dropout(dropout_params['input_layer_rate']))\n",
    "\n",
    "    # Add a pooling layer\n",
    "    model.add(MaxPooling2D(\n",
    "        pool_size = (2, 2),\n",
    "        strides =  (2, 2)))\n",
    "\n",
    "    # Add the second convolution layer\n",
    "    model.add(Conv2D(\n",
    "        filters = 50,\n",
    "        kernel_size = (5, 5),\n",
    "        padding = \"same\"))\n",
    "    \n",
    "    if batch_norm_params['prev_to_act'] and batch_norm_params['conv_layers']:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if dropout_params['prev_to_act'] and dropout_params['conv_layers']:\n",
    "        model.add(Dropout(dropout_params['conv_layers_rate']))\n",
    "\n",
    "    # Add a ReLU activation function\n",
    "    model.add(Activation(\n",
    "        activation = \"relu\"))\n",
    "    \n",
    "    if not batch_norm_params['prev_to_act'] and batch_norm_params['conv_layers']:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if not dropout_params['prev_to_act'] and dropout_params['conv_layers']:\n",
    "        model.add(Dropout(dropout_params['conv_layers_rate']))\n",
    "\n",
    "    # Add a second pooling layer\n",
    "    model.add(MaxPooling2D(\n",
    "        pool_size = (2, 2),\n",
    "        strides = (2, 2)))\n",
    "\n",
    "    # Flatten the network\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add a fully-connected hidden layer\n",
    "    model.add(Dense(500))\n",
    "    \n",
    "    if batch_norm_params['prev_to_act'] and batch_norm_params['hidden_layers']:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if dropout_params['prev_to_act'] and dropout_params['hidden_layers']:\n",
    "        model.add(Dropout(dropout_params['hidden_layers_rate']))\n",
    "\n",
    "    # Add a ReLU activation function\n",
    "    model.add(Activation(\n",
    "        activation = \"relu\"))\n",
    "    \n",
    "    if not batch_norm_params['prev_to_act'] and batch_norm['hidden_layers']:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    if not dropout_params['prev_to_act'] and dropout_params['hidden_layers']:\n",
    "        model.add(Dropout(dropout_params['hidden_layers_rate']))\n",
    "\n",
    "    # Add a fully-connected output layer\n",
    "    model.add(Dense(10))\n",
    "\n",
    "    # Add a softmax activation function\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    # Compile the network\n",
    "    model.compile(\n",
    "        loss = \"categorical_crossentropy\", \n",
    "        optimizer = SGD(lr = 0.01),\n",
    "        metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_le_net_5(X_train, y_train, params):  \n",
    "    if batch_norm_params['input_layer']:\n",
    "        if batch_norm_params['input_norm_type'] == 'standard_norm':\n",
    "            layer = preprocessing.Normalization()\n",
    "\n",
    "            layer.adapt(X_train)\n",
    "\n",
    "            X_train = layer(X_train)\n",
    "        \n",
    "    model = build_lenet_5(params['batch_norm_params'], params['dropout_params'])\n",
    "    \n",
    "    model.fit(X_train, y_train, batch_size=params['batch_size'], epochs=params['nb_epoch'], verbose=params['verbose'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_le_net_5(model, X_test, y_test, params):\n",
    "    (loss, accuracy) = model.evaluate(test_data, test_labels, batch_size=params['batch_size'], verbose=params['verbose'])\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, n_classes, data_dim = generate_mnist_data(flatten=False, newaxis=True)\n",
    "\n",
    "assert X_train.shape == (60000, 28, 28, 1)\n",
    "assert X_test.shape == (10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_norm_params = {\n",
    "    'input_layer': True, \n",
    "    'input_norm_type': 'standard_norm', # Use standard normalization for input\n",
    "    'conv_layers': True,\n",
    "    'hidden_layers' : True,\n",
    "    'prev_to_act': True # Place batch norm layer before activation\n",
    "}\n",
    "\n",
    "# Don't apply Dropout now\n",
    "dropout_params = {\n",
    "    'input_layer': False,\n",
    "    'conv_layers': False,\n",
    "    'hidden_layers' : False,\n",
    "    'prev_to_act': True\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'batch_size': 128,\n",
    "    'nb_epoch': 20,\n",
    "    'verbose': 1,\n",
    "    'batch_norm_params': batch_norm_params,\n",
    "    'dropout_params': dropout_params\n",
    "}\n",
    "\n",
    "model = train_le_net_5(X_train, y_train, params)\n",
    "test_acc = evaluate_le_net_5(model, X_test, y_test, params)\n",
    "\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use batch norm for input as well\n",
    "batch_norm_params['input_norm_type']: 'batch_norm'\n",
    "\n",
    "# Reassign batch_norm_params\n",
    "params['batch_norm_params'] = batch_norm_params\n",
    "\n",
    "model = train_le_net_5(X_train, y_train, params)\n",
    "test_acc = evaluate_le_net_5(model, X_test, y_test, params)\n",
    "\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now invalidate batch_norm_params\n",
    "batch_norm_params = {\n",
    "    'input_layer': False, # Use standard norm instead \n",
    "    'conv_layers': False,\n",
    "    'hidden_layers' : False,\n",
    "    'prev_to_act': True # Place batch norm layer before activation\n",
    "}\n",
    "\n",
    "# Apply Dropout now\n",
    "dropout_params = {\n",
    "    'input_layer': True,\n",
    "    'conv_layers': True,\n",
    "    'hidden_layers' : True,\n",
    "    'prev_to_act': True,\n",
    "    'input_layer_rate': 0.2,\n",
    "    'conv_layers_rate': 0.5,\n",
    "    'hidden_layers_rate': 0.5\n",
    "}\n",
    "\n",
    "# Reassign both batch_norm_params and dropout_params\n",
    "params['batch_norm_params'] = batch_norm_params\n",
    "params['dropout_params'] = dropout_params\n",
    "\n",
    "model = train_le_net_5(X_train, y_train, params)\n",
    "test_acc = evaluate_le_net_5(model, X_test, y_test, params)\n",
    "\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use batch_norm_params again\n",
    "batch_norm_params = {\n",
    "    'input_layer': True, \n",
    "    'input_norm_type': 'batch_norm', # Use batch norm to normalize input\n",
    "    'conv_layers': True,\n",
    "    'hidden_layers' : True,\n",
    "    'prev_to_act': True # Place batch norm layer before activation\n",
    "}\n",
    "\n",
    "# Reassign batch_norm_params\n",
    "params['batch_norm_params'] = batch_norm_params\n",
    "\n",
    "model = train_le_net_5(X_train, y_train, params)\n",
    "test_acc = evaluate_le_net_5(model, X_test, y_test, params)\n",
    "\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 - Learning Rate, Batch Size, FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - Linear Separability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_class_1 = np.array([-1, 1, -3, 4])\n",
    "y_class_1 = np.array([-1, 1, -3, 4])\n",
    "\n",
    "x_class_2 = np.array([-1, 1, -5, 4])\n",
    "y_class_2 = np.array([1, -1, 2, -8])\n",
    "\n",
    "plt.plot(x_class_1, y_class_1, 'o', color='green', label='Class 1')\n",
    "ax = plt.gca()\n",
    "ax.set(xlabel = 'x1', ylabel = 'x2')\n",
    "\n",
    "plt.plot(x_class_2, y_class_2, 'o', color='blue', label='Class 2')\n",
    "ax = plt.gca()\n",
    "ax.set(xlabel = 'x1', ylabel = 'x2')\n",
    "\n",
    "plt.title('2D Scatter Plot for points (x1, x2) in Class 1 (Green) and Class 2 (Blue)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the first set of points belongs to Class 1 and the other set belongs to Class 2.\n",
    "\n",
    "Green points denote Class 1 while Blue Points denote Class 2. $x_1$ is the horizontal dimension while $x_2$ is the vertical one.\n",
    "\n",
    "We can see from the plot above that the dataset $\\text{IS NOT LINEARLY SEPARABLE}$ using any linear function/classifier, given only two features $ x_1$ and $x_2$ (without further transformation into a higher space). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the points from Class 1 and Class 2, we could see that:\n",
    "1. $x_1$ and $x_2$ in Class 1 are of the same sign\n",
    "2. $x_1$ and $x_2$ in Class 2 are of the opposite sign\n",
    "\n",
    "Thus, we could propose some $z$ such as: $${z = x_1 * x_2}$$\n",
    "\n",
    "This is linearly separable because Class 1 will all have positive values of $z$ while Class 2 will all have negative values of $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a 1D plane which plots $z$ for Class 1 and Class 2. Green lines denote Class 1 while Blue Points denote Class 2.\n",
    "\n",
    "The separating hyperplane (or point, because this is 1D) is simply $z=c$, where $c$ is any constant satisfying $-1 < c < 1$. I pick $c=0$, which is denoted by the thick and short red line in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_class_1 = np.multiply(x_class_1, y_class_1)\n",
    "z_class_2 = np.multiply(x_class_2, y_class_2)\n",
    "\n",
    "z_class_1 = np.repeat(z_class_1, 2)\n",
    "z_class_2 = np.repeat(z_class_2, 2)\n",
    "\n",
    "plt.figure()\n",
    "plt.hlines(1, -35, 20)  \n",
    "\n",
    "plt.eventplot(z_class_1, orientation='horizontal', colors='green', linelengths=0.1, linewidths=2.5, label='Class 1')\n",
    "plt.eventplot(z_class_2, orientation='horizontal', colors='blue', linelengths=0.1, linewidths=2.5, label='Class 2')\n",
    "\n",
    "plt.eventplot([-35, 20], orientation='horizontal', colors='black', linelengths=1, linewidths=1)\n",
    "plt.eventplot([0], orientation='horizontal', colors='red', linelengths=0.2, linewidths=5, label='Separating Hyperplane')\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "plt.title('1D Plot for points (z) in Class 1 (Green) and Class 2 (Blue)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for Problem 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question\n",
    "$$$$\n",
    "Explain the importance of nonlinear transformations in classification problems.\n",
    "$$$$\n",
    "Answer\n",
    "$$$$\n",
    "Nonlinear transformations are important to help classifier create a decision boundary for dataset that are not linearly separable. For example, in scikit-learn, there is a package called Kernel SVM which use a Kernel to project the non-linearly separable data in some lower dimension to linearly separable data in some higher dimensions so that the data points belonging to different classes are allocated to different dimensions.\n",
    "\n",
    "Reference: https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/#:~:text=Rather%2C%20a%20modified%20version%20of,are%20allocated%20to%20different%20dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Bias Variance Tradeoff, Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E[MSE] = E[\\frac{1}{t} \\sum_{i=1}^t(f(x_i) + \\epsilon - g(x_i))^2] $$\n",
    "\n",
    "Introduce terms $E[g(x_i]$ that will cancel each other:\n",
    "$$ E[MSE] = E[\\frac{1}{t} \\sum_{i=1}^t (f(x_i) + \\epsilon - g(x_i) + E[g(x_i)] - E[g(x_i)])^2] $$\n",
    "\n",
    "Using linearity in expectation:\n",
    "$$ E[MSE] = E[\\frac{1}{t} \\sum_{i=1}^t (f(x_i)-E[g(x_i)])^2 + \\frac{1}{t} \\sum_{i=1}^t E[\\epsilon^2] + \\frac{1}{t} \\sum_{i=1}^t E[(E[g(x_i)] - g(x_i))^2] + \\frac{2}{t} \\sum_{i=1}^t E[\\epsilon(f(x_i) - E[g(x_i)])] + \\frac{2}{t} \\sum_{i=1}^t E[\\epsilon(E[g(x_i)] - g(x_i))] + \\frac{2}{t} \\sum_{i=1}^t E[((f(x_i) - E[g(x_i))(E[g(x_i)] - g(x_i))]] $$\n",
    "\n",
    "Notice that $E[g(x_i)] = g(x_i)$, so that lefts us with:\n",
    "$$ E[MSE] = E[\\frac{1}{t} \\sum_{i=1}^t (f(x_i)-E[g(x_i)])^2 + \\frac{1}{t} \\sum_{i=1}^t E[\\epsilon^2] + \\frac{1}{t} \\sum_{i=1}^t E[(E[g(x_i)] - g(x_i))^2]$$\n",
    "\n",
    "Again, using linearity in expectation:\n",
    "$$ E[MSE] = E[\\frac{1}{t} \\sum_{i=1}^t (f(x_i)-E[g(x_i)])^2] + E[\\frac{1}{t} \\sum_{i=1}^t E[\\epsilon^2]] + E[\\frac{1}{t} \\sum_{i=1}^t E[(E[g(x_i)] - g(x_i))^2]$$\n",
    "\n",
    "$$ E[MSE] = \\frac{1}{t} \\sum_{i=1}^t (f(x_i)-E[g(x_i)])^2 + \\frac{1}{t} \\sum_{i=1}^t E[\\epsilon^2] + \\frac{1}{t} \\sum_{i=1}^t E[(E[g(x_i)] - g(x_i))^2$$\n",
    "\n",
    "$$ E[MSE] = \\frac{1}{t} \\sum_{i=1}^t (f(x_i)-E[g(x_i)])^2 + \\frac{1}{t} \\sum_{i=1}^t E[(E[g(x_i)] - g(x_i))^2] + \\frac{1}{t}(t) E[\\epsilon^2]$$\n",
    "\n",
    "$$ E[MSE] = \\frac{1}{t} \\sum_{i=1}^t (f(x_i)-E[g(x_i)])^2 + \\frac{1}{t} \\sum_{i=1}^t E[(E[g(x_i)] - g(x_i))^2] + E[\\epsilon^2]$$\n",
    "\n",
    "Finally:\n",
    "$$ E[MSE] = Bias[g(x)]^2 + Var[g(x)] + Noise $$\n",
    "\n",
    "where:\n",
    "1. $$ Bias[g(x)]^2 = \\frac{1}{t} \\sum_{i=1}^t (f(x_i)-E[g(x_i)])^2 $$\n",
    "2. $$ Var[g(x)] = \\frac{1}{t} \\sum_{i=1}^t E[(E[g(x_i)] - g(x_i))^2 $$\n",
    "3. $$ Noise = E[\\epsilon^2] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The black line plot depicts $f(x)$ while the red dots (20 in total) are the samples drawn from $y(x)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_x(x):\n",
    "    return x + np.sin((3/2)*x)\n",
    "\n",
    "x_smooth = np.arange(0, 10, 0.01)\n",
    "f_x_plot_dots = f_x(x_smooth)\n",
    "\n",
    "def generate_sample_from_y(x_rand, use_noise):\n",
    "    y = f_x(x_rand)\n",
    "    \n",
    "    if use_noise:\n",
    "        y = y + np.random.normal(0, 0.3, len(x_rand))\n",
    "        \n",
    "    return y\n",
    "    \n",
    "x_rand = np.random.uniform(0, 10, 20)\n",
    "y_x_plot_dots = generate_sample_from_y(x_rand, True)\n",
    "\n",
    "\n",
    "plt.plot(x_smooth, f_x_plot_dots, 'o', color='black', markersize=2.5, label='f(x)')\n",
    "plt.plot(x_rand, y_x_plot_dots, 'o', color='red', markersize=7.5, label='y(x)')\n",
    "\n",
    "plt.title('Smooth Line Plot for f(x): Black Line and Scatter Plot for y(x): Red Dots using 20 random points')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reshape_x_smooth = x_smooth.reshape(-1, 1)\n",
    "\n",
    "g_1 = make_pipeline(PolynomialFeatures(1), LinearRegression())\n",
    "g_1.fit(reshape_x_smooth, f_x(x_smooth).reshape(-1, 1))\n",
    "\n",
    "g_3 = make_pipeline(PolynomialFeatures(3), LinearRegression())\n",
    "g_3.fit(reshape_x_smooth, f_x(x_smooth).reshape(-1, 1))\n",
    "\n",
    "g_10 = make_pipeline(PolynomialFeatures(10), LinearRegression())\n",
    "g_10.fit(reshape_x_smooth, f_x(x_smooth).reshape(-1, 1))\n",
    "\n",
    "plt.plot(x_smooth, f_x_plot_dots, 'o', color='black', markersize=2.5, label='f(x)')\n",
    "plt.plot(x_smooth, g_1.predict(reshape_x_smooth), 'o', color='blue', markersize=1, label='g1(x)')\n",
    "plt.plot(x_smooth, g_3.predict(reshape_x_smooth), 'o', color='green', markersize=1, label='g3(x)')\n",
    "plt.plot(x_smooth, g_10.predict(reshape_x_smooth), 'o', color='red', markersize=1, label='g10(x)')\n",
    "\n",
    "plt.title('Plot A: Blue depicts g1(x), Green depicts g3(x), Red depicts g10(x) and Black depicts f(x)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_smooth, f_x_plot_dots, 'o', color='black', markersize=2.5, label='f(x)')\n",
    "plt.plot(x_smooth, g_1.predict(reshape_x_smooth), 'o', color='blue', markersize=1, label='g1(x)')\n",
    "plt.plot(x_smooth, g_3.predict(reshape_x_smooth), 'o', color='green', markersize=1, label='g3(x)')\n",
    "\n",
    "plt.title('Plot B: Blue depicts g1(x), Green depicts g3(x) and Black depicts f(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first plot, Plot A, we can see that $g_1(x)$ (blue line) and $g_2(x)$ (green line) are underfitting while $g_{10}(x)$ (red line) is overfitting. $f(x)$, which is the black line, is fully overlayed by $g_{10}(x)$, the red line.\n",
    "\n",
    "To make it clearer, I provide Plot B, in which I get ride of $g_{10}(x)$, i.e. the red line. We can compare that $g_{10}(x)$ resembles $f(x)$ quiet a lot, indicating overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Generate Datasets and to Simulate the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import polyfit, polyval\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def generate_dataset(n_sample, x_low, x_high, n_dataset, test_frac, seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_rand = np.random.uniform(x_low, x_high, n_sample)\n",
    "    \n",
    "    slice_idx = int(test_frac * n_sample)\n",
    "    \n",
    "    x_train = x_rand[:slice_idx]\n",
    "    x_test = x_rand[slice_idx:]\n",
    "    \n",
    "    # y without noise\n",
    "    y_x_plot_dots = generate_sample_from_y(x_rand, False)\n",
    "    y_train = y_x_plot_dots[:slice_idx]\n",
    "    y_test = y_x_plot_dots[slice_idx:]\n",
    "\n",
    "    # y with noise\n",
    "    y_train_sets_noise = []\n",
    "    y_test_sets_noise = []\n",
    "    \n",
    "    # Generate y_train and y_test\n",
    "    for i in range(0, n_dataset):\n",
    "        y_x_plot_dots_noise = generate_sample_from_y(x_rand, True)\n",
    "\n",
    "        y_train_sets_noise.append(y_x_plot_dots_noise[:slice_idx])    \n",
    "        y_test_sets_noise.append(y_x_plot_dots_noise[slice_idx:])\n",
    "        \n",
    "    return x_train.reshape(-1, 1), x_test.reshape(-1, 1), y_train, y_test, y_train_sets_noise, y_test_sets_noise\n",
    "\n",
    "def compute_error(y_test, y_pred):\n",
    "    y_test = y_test.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    \n",
    "    assert y_test.shape == (10, )\n",
    "    assert y_pred.shape == (10, )\n",
    "    \n",
    "    return mean_squared_error(y_pred, y_test)\n",
    "\n",
    "def simulate(degree_low, degree_high, x_train, x_test, y_train_sets_noise, y_test_sets_noise):\n",
    "    x_train = x_train.flatten() # polyfit needs 1D vector\n",
    "    \n",
    "    y_preds_train = [] # (15, 100, 40, 1) \n",
    "    y_preds_test = [] # (15, 100, 10, 1)\n",
    "    test_err = [] # (15, 100, 10, 1)\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    for degree in range(degree_low - 1, degree_high):\n",
    "        y_preds_train.append([])\n",
    "        y_preds_test.append([])\n",
    "        test_err.append([])\n",
    "    \n",
    "    for i in range(0, len(y_train_sets_noise)):\n",
    "        y_train_noise = y_train_sets_noise[i].reshape(-1, 1)\n",
    "        y_test_noise = y_test_sets_noise[i].reshape(-1, 1)\n",
    "\n",
    "        for degree in range(degree_low - 1, degree_high):\n",
    "            model = None\n",
    "            # IMPORTANT: Increment the degree by 1 shere so it depicts the real degree\n",
    "            # Funny story: I spend almost one hour debugging this offset bug\n",
    "            model = polyfit(x_train, y_train_noise, degree + 1)\n",
    "            models.append(model)\n",
    "            \n",
    "            y_pred_test = polyval(model, x_test)\n",
    "            y_preds_test[degree].append(y_pred_test)\n",
    "\n",
    "            # IMPORTANT: For error, use y WITH NOISE\n",
    "            test_err[degree].append(compute_error(y_test_noise, y_pred_test))\n",
    "            \n",
    "    return models, np.array(y_preds_test), np.array(test_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Compute Bias, Variance and Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_squared_bias(y_test, y_preds_test):\n",
    "    # E[g(x)]\n",
    "    avg_y_preds_test = y_preds_test.mean(axis=0)\n",
    "    \n",
    "    # Sanity check\n",
    "    assert avg_y_preds_test.shape == (10, 1)\n",
    "    \n",
    "    # (E[g(x)] - f(x))**2\n",
    "    # IMPORTANT: Use y WITHOUT NOISE\n",
    "    # IMPORTANT: FLATTEN the average so it becomes (10, )\n",
    "    return mean_squared_error(avg_y_preds_test.flatten(), y_test)\n",
    "\n",
    "def compute_variance(y_preds_test):\n",
    "    # IMPORTANT: Remove third dimension, (100, 10, 1) -> (100, 10)\n",
    "    rows, cols = len(y_preds_test), len(y_preds_test[0])\n",
    "    y_preds_test = y_preds_test.flatten().reshape(rows, cols)\n",
    "    \n",
    "    # E[g(x)]\n",
    "    avg_y_preds_test = np.mean(y_preds_test, axis=0)\n",
    "    # IMPORTANT: Tile to make duplicates, (10, ) -> (100, 10)\n",
    "    avg_y_preds_test = np.tile(avg_y_preds_test, (100, 1))\n",
    "    \n",
    "    # (g(x) - E[g(x)])**2\n",
    "    return mean_squared_error(y_preds_test, avg_y_preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Routine to Get the Stats for Each Polynomial Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, y_train_sets_noise, y_test_sets_noise = generate_dataset(50, 0, 10, 100, 0.8, 82)\n",
    "\n",
    "models, y_preds_test, test_err = simulate(1, 15, x_train, x_test, y_train_sets_noise, y_test_sets_noise)\n",
    "\n",
    "# Sanity check\n",
    "assert y_preds_test.shape == (15, 100, 10, 1)\n",
    "assert test_err.shape == (15, 100)\n",
    "\n",
    "avg_squared_biases = []\n",
    "avg_test_errs = []\n",
    "variances = []\n",
    "\n",
    "for degree in range(0, 15):\n",
    "    curr_y_pred_test = y_preds_test[degree]\n",
    "    \n",
    "    avg_squared_biases.append(compute_squared_bias(y_test, curr_y_pred_test))\n",
    "    avg_test_errs.append(np.mean(test_err[degree]))\n",
    "    variances.append(compute_variance(curr_y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Testing Error, Bias and Variance against Model Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x_plots = np.arange(1, 16, 1)\n",
    "y_plots = avg_squared_biases\n",
    "ax.plot(x_plots, y_plots, color='blue', label='Squared Bias')\n",
    "\n",
    "x_plots = np.arange(1, 16, 1)\n",
    "y_plots = avg_test_errs\n",
    "ax.plot(x_plots, y_plots, color='red', label='Test Error')\n",
    "\n",
    "x_plots = np.arange(1, 16, 1)\n",
    "y_plots = variances\n",
    "ax.plot(x_plots, y_plots, color='green', label='Variances')\n",
    "\n",
    "ax.set_xlabel('Polynomial Degree')         \n",
    "\n",
    "ax.set_title('Testing Error Relationship to Bias and Variance')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_performing_degree = np.argmin(avg_test_errs) + 1\n",
    "best_performing_degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing model is the one with the lowest testing error, which is Polynomial Degree 8. We can also see that from the graph that this model seems to have a good balance between its bias and variance at Polynomial Degree 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression is Used to Apply L2 Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "degree = 10\n",
    "\n",
    "# Polynomial Degree 10 model from 2.4\n",
    "model_10 = models[degree - 1] \n",
    "\n",
    "# Ridge Regression, e.g. LinearRegression with L2 regularization\n",
    "# Use alpha=1.0, meaning we optimize bias and norm equally\n",
    "model_10_L2 = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=1.0))\n",
    "\n",
    "y_preds_test_L2 = []\n",
    "test_err_L2 = []\n",
    "\n",
    "# Simulation Loop\n",
    "for i in range(0, len(y_train_sets_noise)):\n",
    "    y_train_noise = y_train_sets_noise[i].reshape(-1, 1)\n",
    "    y_test_noise = y_test_sets_noise[i].reshape(-1, 1)\n",
    "\n",
    "    model_10_L2.fit(x_train, y_train_noise)\n",
    "\n",
    "    y_pred_test = model_10_L2.predict(x_test)\n",
    "    y_preds_test_L2.append(y_pred_test)\n",
    "\n",
    "    # IMPORTANT: For error, use y WITH NOISE\n",
    "    test_err_L2.append(compute_error(y_test_noise, y_pred_test))\n",
    "    \n",
    "y_preds_test_L2 = np.array(y_preds_test_L2)\n",
    "test_err_L2 = np.array(test_err_L2)\n",
    "\n",
    "avg_squared_bias_L2 = compute_squared_bias(y_test, y_preds_test_L2)\n",
    "avg_test_err_L2 = np.mean(test_err_L2)\n",
    "variance_L2 = compute_variance(y_preds_test_L2)\n",
    "\n",
    "print('Model without Regularization:')\n",
    "print('Squared Bias: {}'.format(avg_squared_biases[degree-1]))\n",
    "print('MSE: {}'.format(avg_test_errs[degree-1]))\n",
    "print('Variance: {}'.format(variances[degree-1]))\n",
    "\n",
    "print('-----------------------------')\n",
    "print('-----------------------------')\n",
    "\n",
    "print('Model with L2 Regularization:')\n",
    "print('Squared Bias: {}'.format(avg_squared_bias_L2))\n",
    "print('MSE: {}'.format(avg_test_err_L2))\n",
    "print('Variance: {}'.format(variance_L2))\n",
    "\n",
    "print('-----------------------------')\n",
    "print('-----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation for Bias and MSE Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the report above, we could see:\n",
    "    \n",
    "1. The regularized model have a significantly higher bias ($0.0694$ vs $0.0003$)\n",
    "2. The regularized model also has a higher MSE ($0.1744$ vs $0.1174$)\n",
    "3. However, the regularized model has a lower variance, although not significant ($0.0154$ vs $0.0257$)\n",
    "\n",
    "The explanations are:\n",
    "1. The regularized model has lower variance because it takes into account the solution norm, L2 to be specific, into its loss function. Its loss function is roughly: $||y - Xw||^2_2 + alpha * ||w||^2_2$ where it is an addition of bias (like normal linear regression) and the L2 norm of the solution $w$ to some extent of alpha. I use alpha equals 1 that means we optimize both terms equally. Thus, not only the regularized model minimizes the bias, but it tries to also minimize the L2 norm at the same time, resulting in a simpler model with lower variance.\n",
    "2. The consequences of having to minimize both bias and L2 norms are that we might not get the solution with optimal (minimum) bias. Hence, with L2 regularization, we frequently have lower bias and MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two datasets picked:\n",
    "    \n",
    "1. energy-efficiency (https://www.openml.org/d/1472)\n",
    "2. optdigits (https://www.openml.org/d/28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "d1_path = 'data/energy-efficiency.csv'\n",
    "d1_name = 'Energy Efficiency'\n",
    "\n",
    "t_df = pd.read_csv(d1_path)\n",
    "t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2_path = 'data/optdigits.csv'\n",
    "d2_name = 'Opt Digits'\n",
    "\n",
    "i_df = pd.read_csv(d2_path)\n",
    "i_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Summarize Attributes from a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attribute_summary(df, class_column, dname):\n",
    "    df_info = {}\n",
    "\n",
    "    df_info['num_features'] = len(df.columns) - 1 # excluding label column\n",
    "    df_info['num_instances'] = len(df)\n",
    "    df_info['num_classes'] = len(np.unique(df[class_column]))\n",
    "\n",
    "    df_info['num_numerical_features'] = 0\n",
    "    df_info['num_categorical_features'] = 0\n",
    "    \n",
    "    df_info['categorical_indexes'] = []\n",
    "\n",
    "    idx = 0\n",
    "    for column in df:\n",
    "        if column != class_column:\n",
    "            if str(df[column].dtypes) == 'category':\n",
    "                df_info['num_categorical_features'] = df_info['num_categorical_features'] + 1\n",
    "                df_info['categorical_indexes'].append(idx)\n",
    "            else:\n",
    "                df_info['num_numerical_features'] = df_info['num_numerical_features'] + 1\n",
    "            \n",
    "            idx = idx + 1\n",
    "\n",
    "    # Some sanity check\n",
    "    assert df_info['num_numerical_features'] + df_info['num_categorical_features'] == df_info['num_features']\n",
    "\n",
    "    print('Summary of {} dataset attributes: '.format(dname))\n",
    "    print('Number of features: {}'.format(df_info['num_features']))\n",
    "    print('Number of instances: {}'.format(df_info['num_instances']))\n",
    "    print('Number of classes: {}'.format(df_info['num_classes']))\n",
    "    print('Number of numerical features: {}'.format(df_info['num_numerical_features']))\n",
    "    print('Number of categorical features: {}'.format(df_info['num_categorical_features']))\n",
    "    \n",
    "    return df_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribute Summary for Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_classname = 'y1'  \n",
    "\n",
    "# Even though y2 contains integer, but it is stated as a nominal variable\n",
    "# in the dataset explanation (see https://www.openml.org/d/1472)\n",
    "# Thus, I add some string prefix so it will count as nominal\n",
    "# t_df['y2'] = 'str' + t_df['y2'].astype(str)\n",
    "t_df['y2'] = t_df['y2'].astype('category')\n",
    "\n",
    "d1_info = get_attribute_summary(t_df, d1_classname, d1_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribute Summary for Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2_classname = 'class'\n",
    "d2_info = get_attribute_summary(i_df, d2_classname, d2_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Categorical Features in Both Dataset, Both Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = pd.get_dummies(t_df)\n",
    "i_df = pd.get_dummies(i_df)\n",
    "\n",
    "# print(t_df.shape) # (768, 47)\n",
    "# print(i_df.shape) # (5620, 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Both Dataset into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "t_X_train, t_X_test, t_y_train, t_y_test = train_test_split(t_df.drop(columns=[d1_classname]), t_df[d1_classname], test_size=0.2, random_state=28)\n",
    "i_X_train, i_X_test, i_y_train, i_y_test = train_test_split(i_df.drop(columns=[d2_classname]), i_df[d2_classname], test_size=0.2, random_state=28)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate 10 Different Train Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ten_subsamples(X_train, y_train):\n",
    "    X_train_sets = []\n",
    "    y_train_sets = []\n",
    "    \n",
    "    for i in range(1, 11):\n",
    "        frac = i * 10 / 100\n",
    "\n",
    "        # Use the same random state so that we get matching X_train and y_train\n",
    "        X_train_sets.append(X_train.sample(frac=frac, random_state=28))\n",
    "        y_train_sets.append(y_train.sample(frac=frac, random_state=28))\n",
    "        \n",
    "    return X_train_sets, y_train_sets\n",
    "\n",
    "t_X_train_sets, t_y_train_sets = generate_ten_subsamples(t_X_train, t_y_train)\n",
    "i_X_train_sets, i_y_train_sets = generate_ten_subsamples(i_X_train, i_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Train Dataset using Random Forests and Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def run_train_scenario(dname, X_train_sets, y_train_sets):\n",
    "    i = 0\n",
    "    \n",
    "    rf_models = []\n",
    "    gb_models = []\n",
    "    \n",
    "    rf_training_times = []\n",
    "    gb_training_times = []\n",
    "    \n",
    "    for X_train, y_train in list(zip(X_train_sets, y_train_sets)):\n",
    "        rf = RandomForestClassifier(n_estimators=20, max_depth=5, random_state=28)\n",
    "        gb = GradientBoostingClassifier(n_estimators=20, max_depth=5, random_state=28)\n",
    "    \n",
    "        start = datetime.now()\n",
    "        rf.fit(X_train, y_train)\n",
    "        rf_training_times.append((datetime.now() - start).total_seconds())\n",
    "        rf_models.append(rf)\n",
    "        \n",
    "        start = datetime.now()\n",
    "        gb.fit(X_train, y_train)\n",
    "        gb_training_times.append((datetime.now() - start).total_seconds())\n",
    "        gb_models.append(gb)\n",
    "        \n",
    "        i = i + 1\n",
    "        total_time_elapsed = rf_training_times[len(rf_training_times) - 1] + gb_training_times[len(gb_training_times) - 1]\n",
    "#         print('Finished {} training for subsample {}, total time elapsed: {}'.format(dname, i, total_time_elapsed))\n",
    "        \n",
    "    return rf_models, gb_models, rf_training_times, gb_training_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Training on Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_rf_models, t_gb_models, t_rf_training_times, t_gb_training_times = run_train_scenario(d1_name, t_X_train_sets, t_y_train_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Training with Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_rf_models, i_gb_models, i_rf_training_times, i_gb_training_times = run_train_scenario(d2_name, i_X_train_sets, i_y_train_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create function to Compute Accuracies on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calculate_test_accuracies(clf_list, X_test, y_test):\n",
    "    accuracies = []\n",
    "    \n",
    "    for clf in clf_list:\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Test Accuracies on Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_rf_accuracies = calculate_test_accuracies(t_rf_models, t_X_test, t_y_test)\n",
    "t_gb_accuracies = calculate_test_accuracies(t_gb_models, t_X_test, t_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Test Accuracy on Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_rf_accuracies = calculate_test_accuracies(i_rf_models, i_X_test, i_y_test)\n",
    "i_gb_accuracies = calculate_test_accuracies(i_gb_models, i_X_test, i_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Test Accuracies and Training Times for Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x_plots = np.arange(10, 110, 10)\n",
    "y_plots = t_rf_accuracies\n",
    "ax.plot(x_plots, y_plots, color='red', label='Random Forests')\n",
    "\n",
    "x_plots = np.arange(10, 110, 10)\n",
    "y_plots = t_gb_accuracies\n",
    "ax.plot(x_plots, y_plots, color='green', label='Gradient Boosting')\n",
    "\n",
    "ax.set_xlabel('Training Data Size (in %)')     \n",
    "ax.set_ylabel('Accuracy')     \n",
    "\n",
    "ax.set_title('Test Accuracies Plot for ' + d1_name + ' Dataset')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x_plots = np.arange(10, 110, 10)\n",
    "y_plots = t_rf_training_times\n",
    "ax.plot(x_plots, y_plots, color='red', label='Random Forests')\n",
    "\n",
    "x_plots = np.arange(10, 110, 10)\n",
    "y_plots = t_gb_training_times\n",
    "ax.plot(x_plots, y_plots, color='green', label='Gradient Boosting')\n",
    "\n",
    "ax.set_xlabel('Training Data Size (in %)')     \n",
    "ax.set_ylabel('Training Time (in seconds)')     \n",
    "\n",
    "ax.set_title('Training Time Plot for ' + d1_name + ' Dataset')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Test Accuracies and Training Times for Dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x_plots = np.arange(10, 110, 10)\n",
    "y_plots = i_rf_accuracies\n",
    "ax.plot(x_plots, y_plots, color='red', label='Random Forests')\n",
    "\n",
    "x_plots = np.arange(10, 110, 10)\n",
    "y_plots = i_gb_accuracies\n",
    "ax.plot(x_plots, y_plots, color='green', label='Gradient Boosting')\n",
    "\n",
    "ax.set_xlabel('Training Data Size (in %)')     \n",
    "ax.set_ylabel('Accuracy')     \n",
    "\n",
    "ax.set_title('Test Accuracies Plot for ' + d2_name + ' Dataset')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x_plots = np.arange(10, 110, 10)\n",
    "y_plots = i_rf_training_times\n",
    "ax.plot(x_plots, y_plots, color='red', label='Random Forests')\n",
    "\n",
    "x_plots = np.arange(10, 110, 10)\n",
    "y_plots = i_gb_training_times\n",
    "ax.plot(x_plots, y_plots, color='green', label='Gradient Boosting')\n",
    "\n",
    "ax.set_xlabel('Training Data Size (in %)')     \n",
    "ax.set_ylabel('Training Time (in seconds)')     \n",
    "\n",
    "ax.set_title('Training Time Plot for ' + d2_name + ' Dataset')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For context, I'm limiting the n_estimators to 20 and max_depth to 3 for both algorithm (Random Forests and Gradient Boosting). \n",
    "\n",
    "There are three main observations that I find:\n",
    "\n",
    "1. For both dataset, the training time of Random Forests is constant, it is not affected by the training data size. Meanwhile, the training time for Gradient Boosting grows linearly with the increase in training data size.\n",
    "\n",
    "2. For dataset with many features (64 features for Opt Digits, Dataset 2), Random Forests doesn't do well, i.e. the increase in training size doesn't boost performance on test set. For dataset with small features (9 features for ), Random Forests are still quite on par with Gradient Boosting (accuracy doesn't differ significantly).\n",
    "\n",
    "3. Gradient Boosting, while being limited to small max_depth and n_estimators, still performs well on both dataset, meaning it is robust to significant increase in features.\n",
    "\n",
    "Thus, in general, Gradient Boosting wins performance wise, i.e. it gives better accuracy, meanwhile Random Forest wins speed wise, i.e. it has shorter training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1\n",
    "$$$$\n",
    "Does true negative matter for both ROC and PR curve? \n",
    "$$$$\n",
    "Answer\n",
    "$$$$\n",
    "True Negative (TN) matters for ROC curve, but not for PR curve. True Negative matters for ROC curve because its x-axis is FPR (False Positive Rate). We know that FPR is obtained as below:\n",
    "\n",
    "$$FPR = \\frac{FP}{N} = \\frac{FP}{FP + TN}$$\n",
    "\n",
    "We can see that TN is involved in the formula, thus TN matters for PR curve.\n",
    "$$$$\n",
    "Question 2\n",
    "$$$$\n",
    "Argue why each point on ROC curve corresponds to a unique point on PR curve?\n",
    "$$$$\n",
    "Answer \n",
    "$$$$\n",
    "Let's limit the scope for binary classification. Recall the concept that a point on ROC curve defines a certain, unique, confusion matrix given that the dataset is fixed, i.e. we know how many positive and negative examples are there in the dataset.\n",
    "$$$$\n",
    "In PR curve, TN is not used, so each point in PR curve might lead to some different confusion matrices. But, if the dataset is fixed, and we know the other three statistics, which are TP, FP and FN, then TN is unique. If TN is unique, then it is proved that each point on ROC curve corresponds to a unique point on PR. One additional condition would be that the Recall of the model should be greater than zero, otherwise we couldn't compute FP and consequently, TN is not unique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use wdbc dataset (https://www.openml.org/d/1510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4_path = 'data/wdbc.csv'\n",
    "d4_name = 'WDBC'\n",
    "d4_classname = 'Class'\n",
    "\n",
    "df4 = pd.read_csv(d4_path)\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribute Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_attribute_summary(df4, d4_classname, d4_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[d4_classname].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Change label from {1, 2} to {1, 0}. Notice that I do this because Class 1 has more instances, so I assume it's the positive one.\n",
    "2. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[d4_classname] = df4[d4_classname].astype('int')\n",
    "df4[d4_classname] = df4[d4_classname].replace(2, 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df4.drop(columns=[d4_classname]), df4[d4_classname], test_size=0.2, random_state=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Distribution in Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on Both Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I limit the n_estimators to 3 for Adaboost and max_iter to 20 for Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "a_clf = AdaBoostClassifier(n_estimators=3, random_state=28)\n",
    "a_clf.fit(X_train, y_train)\n",
    "\n",
    "l_clf = LogisticRegression(max_iter=20, random_state=28)\n",
    "l_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Prediction and Its Probabilities (Only for Positive/Majority Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_pred_probs = a_clf.predict_proba(X_test) #192, 2\n",
    "l_pred_probs = l_clf.predict_proba(X_test) #192, 2\n",
    "\n",
    "a_pos_probs = a_pred_probs[:, 1]\n",
    "l_pos_probs = l_pred_probs[:, 1]\n",
    "\n",
    "a_y_preds = a_clf.predict(X_test)\n",
    "l_y_preds = l_clf.predict(X_test)\n",
    "\n",
    "# An all positive classifier, always predicts 1\n",
    "ap_pos_probs = [float(1.0) for _ in range(len(y_test))]\n",
    "ap_y_preds = [int(1) for _ in range(len(y_test))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "ap_fp_rate, ap_tp_rate, _ = roc_curve(y_test, ap_pos_probs)\n",
    "a_fp_rate, a_tp_rate, _ = roc_curve(y_test, a_pos_probs)\n",
    "l_fp_rate, l_tp_rate, _ = roc_curve(y_test, l_pos_probs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x_plots = ap_fp_rate\n",
    "y_plots = ap_tp_rate\n",
    "ax.plot(x_plots, y_plots, color='red', marker='.', label='All Positive', linestyle='dashed')\n",
    "\n",
    "x_plots = l_fp_rate\n",
    "y_plots = l_tp_rate\n",
    "ax.plot(x_plots, y_plots, color='blue', marker='.', label='Logistic')\n",
    "\n",
    "x_plots = a_fp_rate\n",
    "y_plots = a_tp_rate\n",
    "ax.plot(x_plots, y_plots, color='green', marker='.', label='Adaboost')\n",
    "\n",
    "x_plots = l_fp_rate\n",
    "y_plots = l_tp_rate\n",
    "ax.plot(x_plots, y_plots, color='blue', marker='.', label='Logistic')\n",
    "\n",
    "ax.set_xlabel('False Positive Rate')     \n",
    "ax.set_ylabel('True Positive Rate')     \n",
    "\n",
    "ax.set_title('ROC Curve using Adaboost and Logistic Regression for ' + d1_name + ' Dataset')\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "ap_precision, ap_recall, _ = precision_recall_curve(y_test, ap_pos_probs) \n",
    "a_precision, a_recall, _ = precision_recall_curve(y_test, a_pos_probs)\n",
    "l_precision, l_recall, _ = precision_recall_curve(y_test, l_pos_probs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x_plots = ap_recall\n",
    "y_plots = ap_precision\n",
    "ax.plot(x_plots, y_plots, color='red', marker='.', label='All Positive', linestyle='dashed')\n",
    "\n",
    "x_plots = a_recall\n",
    "y_plots = a_precision\n",
    "ax.plot(x_plots, y_plots, color='green', marker='.', label='Adaboost')\n",
    "\n",
    "x_plots = l_recall\n",
    "y_plots = l_precision\n",
    "ax.plot(x_plots, y_plots, color='blue', marker='.', label='Logistic')\n",
    "\n",
    "ax.set_xlabel('Recall')     \n",
    "ax.set_ylabel('Precision')     \n",
    "\n",
    "ax.set_title('Precision Recall Curve using Adaboost and Logistic Regression for ' + d1_name + ' Dataset')\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "a_auroc = roc_auc_score(y_test, a_pos_probs)\n",
    "l_auroc = roc_auc_score(y_test, l_pos_probs)\n",
    "\n",
    "print('AUROC value for Adaboost: {}'.format(a_auroc))\n",
    "print('AUROC value for Logistic Regression: {}'.format(l_auroc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUROC value for Logistice Regression is slightly greater than that of Adaboost, with ~0.02 difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate AUPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "a_aupr = average_precision_score(y_test, a_pos_probs)\n",
    "l_aupr = average_precision_score(y_test, l_pos_probs)\n",
    "\n",
    "print('AUPR value for Adaboost: {}'.format(a_aupr))\n",
    "print('AUPR value for Logistic Regression: {}'.format(l_aupr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUPR value for Logistice Regression is slightly greater than that of Adaboost, with ~0.025 difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For AUPRG, I'm going to use `prg` package (https://github.com/meeliskull/prg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyprg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate AUPRG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prg.prg import create_prg_curve, calc_auprg\n",
    "\n",
    "a_prg_curve = create_prg_curve(y_test, a_pos_probs)\n",
    "l_prg_curve = create_prg_curve(y_test, l_pos_probs)\n",
    "\n",
    "a_auprg = calc_auprg(a_prg_curve)\n",
    "l_auprg = calc_auprg(l_prg_curve)\n",
    "\n",
    "print('AUPRG value for Adaboost: {}'.format(a_auprg))\n",
    "print('AUPRG value for Logistic Regression: {}'.format(l_auprg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUROC value for Logistice Regression is slightly greater than that of Adaboost, with ~0.01 difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question\n",
    "$$$$\n",
    "Do you agree with the conclusion of\n",
    "NIPS paper that practitioners should use PR gain curves rather than PR curves?\n",
    "$$$$\n",
    "Answer\n",
    "$$$$\n",
    "Yes, I agree with the author. There are several reasons:\n",
    "\n",
    "1. The author experimentally proves that the conventional AUPR is not accurate in measuring the expected F1 score, i.e. it can favor models with lower expected F1 score amongst other models.\n",
    "\n",
    "2. AUPRG is an easier metric to calculate, compared to AUPR, due to linear interpolation.\n",
    "\n",
    "3. Traditional PR curves don't have some properties that are desirable (like that of ROC). One example would be that the PR curve has some uninterpretable areas. AUPR is not meaningful because it's merely a geometric expected precision when uniformly varying the recall. Also, as we could see in 4.2, PR curve has unreachable region at the lower RHS (right hand side) of the plot, which area depends on the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elem: [1, 1]\n",
      "4 -4\n",
      "8 0\n",
      "Output: 1\n",
      "---------------------------------------\n",
      "Elem: [5, -8]\n",
      "-6 6\n",
      "-12 0\n",
      "Output: -1\n",
      "---------------------------------------\n",
      "Elem: [0, 0]\n",
      "0 0\n",
      "0 0\n",
      "Output: 1\n",
      "---------------------------------------\n",
      "Elem: [-1, -1]\n",
      "-4 4\n",
      "-8 0\n",
      "Output: -1\n",
      "---------------------------------------\n",
      "Elem: [1, -1]\n",
      "0 0\n",
      "0 0\n",
      "Output: 1\n",
      "---------------------------------------\n",
      "Elem: [-8, 5]\n",
      "-6 6\n",
      "-12 0\n",
      "Output: -1\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def linear(val):\n",
    "    return val\n",
    "\n",
    "def sign(val):\n",
    "    if val >=0 :\n",
    "        return 1\n",
    "    \n",
    "    return -1\n",
    "\n",
    "def relu(val):\n",
    "    if val < 0:\n",
    "        return 0\n",
    "\n",
    "    return val\n",
    "\n",
    "def input_layer_1(x_1, x_2):\n",
    "    elem_1 = 2*x_1 + 2*x_2\n",
    "    elem_2 = -1 * elem_1\n",
    "    \n",
    "    x_1, x_2 = linear(elem_1), linear(elem_2)\n",
    "    print(x_1, x_2)\n",
    "\n",
    "    return x_1, x_2\n",
    "\n",
    "def hidden_layer_1(x_1, x_2):\n",
    "    elem_1 = x_1 - x_2\n",
    "    elem_2 = -1 * (x_1 + x_2)\n",
    "    \n",
    "    x_1, x_2 = linear(elem_1), relu(elem_2)\n",
    "    print(x_1, x_2)\n",
    "\n",
    "    return x_1, x_2\n",
    "\n",
    "def output_layer_1(x_1, x_2):\n",
    "    return sign(x_1 + x_2)\n",
    "\n",
    "def nn_1(x_1, x_2):\n",
    "    x_1, x_2 = input_layer_1(x_1, x_2)\n",
    "    x_1, x_2 = hidden_layer_1(x_1, x_2)\n",
    "    \n",
    "    return output_layer_1(x_1, x_2)\n",
    "\n",
    "for elem in [(1, 1), (5, -8), (0, 0), (-1, -1), (1, -1), (-8, 5)]:\n",
    "    print(\"Elem: [{}, {}]\".format(elem[0], elem[1]))\n",
    "    print(\"Output: {}\".format(nn_1(elem[0], elem[1])))  \n",
    "    print('---------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elem: [10, 14]\n",
      "24 23\n",
      "Output: 0\n",
      "---------------------------------------\n",
      "Elem: [10, -1]\n",
      "9 8\n",
      "Output: 0\n",
      "---------------------------------------\n",
      "Elem: [-1, 10]\n",
      "9 8\n",
      "Output: 0\n",
      "---------------------------------------\n",
      "Elem: [-1, -10]\n",
      "0 0\n",
      "Output: 0\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def input_layer_2(x_1, x_2):\n",
    "    elem_1 = x_1 + x_2\n",
    "    elem_2 = x_1 + x_2 - 1\n",
    "    \n",
    "    x_1, x_2 = relu(elem_1), relu(elem_2)\n",
    "    print(x_1, x_2)\n",
    "    \n",
    "    return x_1, x_2\n",
    "\n",
    "def output_layer_2(x_1, x_2):\n",
    "    return relu(x_1 - 2 * x_2)\n",
    "\n",
    "def nn_2(x_1, x_2):\n",
    "    x_1, x_2 = input_layer_2(x_1, x_2)\n",
    "    \n",
    "    return output_layer_2(x_1, x_2)\n",
    "\n",
    "for elem in [(10, 14), (10, -1), (-1, 10), (-1, -10)]:\n",
    "    print(\"Elem: [{}, {}]\".format(elem[0], elem[1]))\n",
    "    print(\"Output: {}\".format(nn_2(elem[0], elem[1])))  \n",
    "    print('---------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
